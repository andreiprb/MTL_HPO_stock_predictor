Starting transfer learning experiment with 50 tickers
Using batch fraction: 0.01
Selected 45 tickers for training (90% of all tickers)
Selected 5 tickers for testing (10% of all tickers)
Train tickers: ['MA', 'KO', 'HD', 'CVX', 'AMZN', 'V', 'META', 'CRM', 'NFLX', 'GOOG', 'NOW', 'INTC', 'DHR', 'AMD', 'SHOP', 'QCOM', 'CRWD', 'LMT', 'PFE', 'AAPL', 'TXN', 'GS', 'ORCL', 'XOM', 'TMO', 'NKE', 'MSFT', 'IBM', 'JPM', 'TSLA', 'VZ', 'PG', 'NVDA', 'UNH', 'DIS', 'ZS', 'WMT', 'ADBE', 'MRNA', 'PYPL', 'BA', 'PEP', 'BABA', 'COST', 'GD']
Test tickers: ['GME', 'GOOGL', 'ABBV', 'BLK', 'T']
Training base model on combined data from training tickers...
Downloading and preparing data for 45 tickers...
Processing MA...
YF.download() has changed argument auto_adjust default to True
Processing KO...
Processing HD...
Processing CVX...
Processing AMZN...
Processing V...
Processing META...
Processing CRM...
Processing NFLX...
Processing GOOG...
Processing NOW...
Processing INTC...
Processing DHR...
Processing AMD...
Processing SHOP...
Processing QCOM...
Processing CRWD...
Processing LMT...
Processing PFE...
Processing AAPL...
Processing TXN...
Processing GS...
Processing ORCL...
Processing XOM...
Processing TMO...
Processing NKE...
Processing MSFT...
Processing IBM...
Processing JPM...
Processing TSLA...
Processing VZ...
Processing PG...
Processing NVDA...
Processing UNH...
Processing DIS...
Processing ZS...
Processing WMT...
Processing ADBE...
Processing MRNA...
Processing PYPL...
Processing BA...
Processing PEP...
Processing BABA...
Processing COST...
Processing GD...
Combined dataset shape: X=(31275, 60, 1), y=(31275,)
Using a third of combined data for hyperparameter tuning
Tuning dataset shape: X=(10320, 60, 1), y=(10320,)
Reloading Tuner from hyper_tuning/_combined_lstm_tuning/tuner0.json
Best Hyperparameters: {'lstm_units_1': 128, 'dropout_rate_1': 0.2, 'lstm_units_2': 128, 'dropout_rate_2': 0.2, 'dense_units': 20, 'learning_rate': 0.001}
Using batch size of 281 (fraction: 0.01)
Epoch 1/10

Original RMSE: 0.34368592225168887

Evaluating on test tickers...

Evaluating on GME...
Downloading data for GME from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Using prediction batch sizes: train=6, test=1
Normalized RMSE: 0.0211
Original RMSE: 1.8148

Evaluating on GOOGL...
Downloading data for GOOGL from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Using prediction batch sizes: train=6, test=1
Normalized RMSE: 0.0401
Original RMSE: 3.8769

Evaluating on ABBV...
Downloading data for ABBV from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Using prediction batch sizes: train=6, test=1
Normalized RMSE: 0.0251
Original RMSE: 2.6122

Evaluating on BLK...
Downloading data for BLK from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Using prediction batch sizes: train=6, test=1
Normalized RMSE: 0.0462
Original RMSE: 27.6213

Evaluating on T...
Downloading data for T from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Using prediction batch sizes: train=6, test=1
Normalized RMSE: 0.0735
Original RMSE: 0.5525

==================================================
Training individual model for GME
==================================================
Tuning hyperparameters for GME...
Downloading data for GME from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Reloading Tuner from hyper_tuning/GME_lstm_tuning/tuner0.json
Best Hyperparameters: {'lstm_units_1': 96, 'dropout_rate_1': 0.5, 'lstm_units_2': 64, 'dropout_rate_2': 0.30000000000000004, 'dense_units': 60, 'learning_rate': 0.001}
Using batch size of 6 (fraction: 0.01)
Epoch 1/10

Original RMSE: 7.2159175511405325
F1 Score (Transfer Learning): 0.2857
F1 Score (Individual Model): 0.3390
F1 Improvement: -15.71%

Comparison for GME:
Transfer Learning: Normalized RMSE: 0.0211, Original RMSE: 1.8148
Individual Model: Normalized RMSE: 0.0837, Original RMSE: 7.2159
Improvement (Normalized): 74.85%
Improvement (Original): 74.85%

==================================================
Training individual model for GOOGL
==================================================
Tuning hyperparameters for GOOGL...
Downloading data for GOOGL from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Reloading Tuner from hyper_tuning/GOOGL_lstm_tuning/tuner0.json
Best Hyperparameters: {'lstm_units_1': 128, 'dropout_rate_1': 0.30000000000000004, 'lstm_units_2': 96, 'dropout_rate_2': 0.5, 'dense_units': 80, 'learning_rate': 0.001}
Using batch size of 6 (fraction: 0.01)
Epoch 1/10

Original RMSE: 9.598686014974763
F1 Score (Transfer Learning): 0.3333
F1 Score (Individual Model): 0.3214
F1 Improvement: 3.70%

Comparison for GOOGL:
Transfer Learning: Normalized RMSE: 0.0401, Original RMSE: 3.8769
Individual Model: Normalized RMSE: 0.0993, Original RMSE: 9.5987
Improvement (Normalized): 59.61%
Improvement (Original): 59.61%

==================================================
Training individual model for ABBV
==================================================
Tuning hyperparameters for ABBV...
Downloading data for ABBV from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Reloading Tuner from hyper_tuning/ABBV_lstm_tuning/tuner0.json
Best Hyperparameters: {'lstm_units_1': 96, 'dropout_rate_1': 0.5, 'lstm_units_2': 128, 'dropout_rate_2': 0.4, 'dense_units': 40, 'learning_rate': 0.001}
Using batch size of 6 (fraction: 0.01)
Epoch 1/10

Original RMSE: 13.801352014014201
F1 Score (Transfer Learning): 0.6067
F1 Score (Individual Model): 0.6353
F1 Improvement: -4.49%

Comparison for ABBV:
Transfer Learning: Normalized RMSE: 0.0251, Original RMSE: 2.6122
Individual Model: Normalized RMSE: 0.1327, Original RMSE: 13.8014
Improvement (Normalized): 81.07%
Improvement (Original): 81.07%

==================================================
Training individual model for BLK
==================================================
Tuning hyperparameters for BLK...
Downloading data for BLK from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Reloading Tuner from hyper_tuning/BLK_lstm_tuning/tuner0.json
Best Hyperparameters: {'lstm_units_1': 128, 'dropout_rate_1': 0.1, 'lstm_units_2': 96, 'dropout_rate_2': 0.1, 'dense_units': 40, 'learning_rate': 0.001}
Using batch size of 6 (fraction: 0.01)
Epoch 1/10

Original RMSE: 89.01265680104238
F1 Score (Transfer Learning): 0.4333
F1 Score (Individual Model): 0.4127
F1 Improvement: 5.00%

Comparison for BLK:
Transfer Learning: Normalized RMSE: 0.0462, Original RMSE: 27.6213
Individual Model: Normalized RMSE: 0.1488, Original RMSE: 89.0127
Improvement (Normalized): 68.97%
Improvement (Original): 68.97%

==================================================
Training individual model for T
==================================================
Tuning hyperparameters for T...
Downloading data for T from 2020-01-01 to 2023-01-01...
Data prepared: X_train: (626, 60, 1), y_train: (626,), X_test: (69, 60, 1), y_test: (69,)
Reloading Tuner from hyper_tuning/T_lstm_tuning/tuner0.json
Best Hyperparameters: {'lstm_units_1': 128, 'dropout_rate_1': 0.2, 'lstm_units_2': 96, 'dropout_rate_2': 0.1, 'dense_units': 60, 'learning_rate': 0.001}
Using batch size of 6 (fraction: 0.01)
Epoch 1/10

Original RMSE: 1.9315947813893517
F1 Score (Transfer Learning): 0.5526
F1 Score (Individual Model): 0.5526
F1 Improvement: 0.00%

Comparison for T:
Transfer Learning: Normalized RMSE: 0.0735, Original RMSE: 0.5525
Individual Model: Normalized RMSE: 0.2571, Original RMSE: 1.9316
Improvement (Normalized): 71.40%
Improvement (Original): 71.40%

==================================================
COMPARISON SUMMARY
==================================================

Transfer learning was better in 5/5 cases
Average improvement (Normalized RMSE): 71.18%
Average improvement (Original RMSE): 71.18%
Transfer learning had better F1 in 2/5 cases
Average improvement (F1 Score): -2.30%
